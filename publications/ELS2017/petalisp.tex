\title{Petalisp: A language for massively parallel computing}

\documentclass[siggraph, review=false]{acmart}
\citestyle{acmauthoryear}
\setcopyright{rightsretained}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003753.10003761.10003762</concept_id>
<concept_desc>Theory of computation~Parallel computing models</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10010175</concept_id>
<concept_desc>Software and its engineering~Parallel programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10010177</concept_id>
<concept_desc>Software and its engineering~Distributed programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
<concept_desc>Software and its engineering~Functional languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10011016</concept_id>
<concept_desc>Software and its engineering~Data flow languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011041.10011044</concept_id>
<concept_desc>Software and its engineering~Just-in-time compilers</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011041.10011048</concept_id>
<concept_desc>Software and its engineering~Runtime environments</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Parallel computing models}
\ccsdesc[500]{Software and its engineering~Parallel programming languages}
\ccsdesc[500]{Software and its engineering~Distributed programming languages}
\ccsdesc[300]{Software and its engineering~Functional languages}
\ccsdesc[300]{Software and its engineering~Data flow languages}
\ccsdesc[300]{Software and its engineering~Just-in-time compilers}
\ccsdesc[100]{Software and its engineering~Runtime environments}

\keywords{High-Performance Computing, Common Lisp}

% DOI
% \acmDOI{10.475/123_4}

% ISBN
% \acmISBN{123-4567-24-567/08/06}

\acmConference[ELS 2017]
{10th European Lisp Symposium}
{April 2017}
{VUB - Vrije Universiteit Brussel, Belgium}

\acmYear{2017}
\copyrightyear{2017}

\author{Marco Heisig}
\affiliation{%
  \institution{FAU Erlangen-N\"urnberg}
  \streetaddress{Cauerstra\ss e 11}
  \city{Erlangen}
  \postcode{91058}
  \country{Germany}
}
\email{marco.heisig@fau.de}

\begin{document}

\begin{abstract}
  Petalisp!
\end{abstract}

\maketitle

\section{Introduction}

The automatic parallelization of computer programs is the holy grail of
parallel computing. The search for this holy grail has spawned many
programming languages, such as High Performance Fortran \cite{HPF}, SISAL
\cite{SISAL}, Fortress \cite{Fortress}, SequenceL \cite{SequenceL}, CM Lisp
\cite{CM-Lisp}, Chapel \cite{Chapel}, Unified Parallel C \cite{UPC} and X10
\cite{X10}. Each of these languages offers a notation for parallel
algorithms and relieves the programmer the tedious and error-prone task of
communicating data between processes. Yet the majority of parallel programs
in industry and scientific computing are written using classical
general-purpose programming languages. This is not surprising, since no
parallelizing compiler can match or outperform a human expert programmer.

But why is that so? Why are parallelizing compilers consistently worse than
human experts? The key difference is not so much the repertoire of known
optimization techniques, but the understanding of the given program. When a
programmer studies a program, she focuses on the high-level algorithm and
data structures. While doing so, she identifies and skips many irrelevant
sections. She might even find some bugs in the original code. This
illustrates that humans have knowledge far beyond the source code.

In contrast, a machine cannot make assumptions about the meaning of a
program. Without any noteworthy intelligence, it is forced to take the
whole program literally. Consequently, the machine has usually zero
knowledge of the high-level behavior of the program. It only sees a jungle
of data and control flow dependencies. Each of these dependencies can
prevent the whole program from parallelization, even if it belongs only to
a superfluous print statement. In other words, it is impossible to develop
a compiler that reliably parallelizes real world programs. The search for
the holy grail is indeed futile.

This does not mean that computers are bad at generating parallel
programs. The source code of a general purpose program is just a horrible
way to convey meaning to such a code generator. Instead we propose a
specialized programming language to express parallel algorithms at a higher
level. This language must be simple enough that every possible combination
of language primitives is intelligible, yet expressive enough to implement
real world algorithms.

In this paper, we describe the design and implementation of the parallel
programming language Petalisp. In section \ref{sec:examples} we show how it
can be applied to several important parallel algorithms.

A peculiarity of Petalisp is its tight integration into the Common Lisp
programming language. Common Lisp programs are used to create Petalisp
programs, and Petalisp operators take ordinary Common Lisp
functions\footnote{albeit undefined behavior occurs if these functions have
  side effects} as arguments. This approach has two advantages. The first
one is that Petalisp itself is a purely functional language, with all
associated benefits for analysis and optimization. The second one is that
Petalisp has access to every feature and library of Common
Lisp. Effectively, Petalisp is an extension of Common Lisp for massively
parallel computing.

\section{The Design of Petalisp}
\label{sec:design}

Petalisp is the product of more than ten design iterations. It started as a
simple domain specific language for numerical computations, until it became
clear that parallel computing must be treated at a more fundamental
level.

\subsection{Strided Arrays}



\subsection{Core Operations}



\section{Examples}
\label{sec:examples}

\section{Implementation}
\label{sec:implementation}

\section{Conclusions}

\bibliographystyle{ACM-Reference-Format}
\bibliography{petalisp}

\end{document}

%  LocalWords:  parallelization Petalisp parallelizing parallelize
